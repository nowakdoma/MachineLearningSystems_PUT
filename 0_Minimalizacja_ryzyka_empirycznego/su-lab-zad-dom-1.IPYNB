{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 1: Minimalizacja ryzyka empirycznego\n",
    "Celem zadania jest zaimplementowanie własnego drzewa decyzyjnego wykorzystującego idee minimalizacji ryzyka empirycznego. \n",
    "\n",
    "### Autor rozwiązania\n",
    "Uzupełnij poniższe informacje umieszczając swoje imię i nazwisko oraz numer indeksu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NAME = \"Dominika Nowak\"\n",
    "ID = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twoja implementacja\n",
    "\n",
    "Twoim celem jest uzupełnić poniższą klasę `TreeNode` tak by po wywołaniu `TreeNode.fit` tworzone było drzewo decyzyjne minimalizujące ryzyko empiryczne. Drzewo powinno wspierać problem klasyfikacji wieloklasowej (jak w przykładzie poniżej). Zaimplementowany algorytm nie musi (ale może) być analogiczny do zaprezentowanego na zajęciach algorytmu dla klasyfikacji. Wszelkie przejawy inwencji twórczej wskazane. **Pozostaw komenatrze w kodzie, które wyjaśniają Twoje rozwiązanie.**\n",
    "\n",
    "Schemat oceniania:\n",
    "- wynik na zbiorze Iris (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 10%: +40%,\n",
    "- wynik na ukrytym zbiorze testowym 1 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 15%: +30%,\n",
    "- wynik na ukrytym zbiorze testowym 2 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 5%: +30%.\n",
    "\n",
    "Niedozwolone jest korzystanie z zewnętrznych bibliotek do tworzenia drzewa decyzyjnego (np. scikit-learn). \n",
    "Możesz jedynie korzystać z biblioteki numpy.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod), o ile będzie się w nim na koniec znajdowała kompletna implementacja klasy `TreeNode` w jednej komórce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TreeNode:\n",
    "\tdef __init__(self):\n",
    "\t\tself.left: TreeNode | None = None  # prawda\n",
    "\t\tself.right: TreeNode | None = None  # niespelniony warunek\n",
    "\t\tself.splitFeature: int | None = None\n",
    "\t\tself.threshold: float | None = None\n",
    "\t\tself.prediction: int | None = None  # wartość liścia -> klasyfikacji\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef giniIndex(y): # w scikit-learn domyslny\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\ty (np.ndarray): Wektor klas.\n",
    "\t\tReturns:\n",
    "            float: Wartość indeksu Giniego.\n",
    "\t\t\"\"\"\n",
    "\t\t# gi = 1 - sum(p(i)^2)\n",
    "\t\t_, counts = np.unique(y, return_counts=True)\n",
    "\t\tprobs = counts / len(y)\n",
    "\t\treturn np.sum(probs**2)\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef entropy(y):\n",
    "\t\t_, counts = np.unique(y, return_counts=True)\n",
    "\t\tprobs = counts / len(y)\n",
    "\t\treturn -np.sum(probs * np.log2(probs))\n",
    "\t\n",
    "\tdef bestSplit(self, data, target):\n",
    "\t\tbestFeature = None\n",
    "\t\tbestTreshold = None\n",
    "\t\tbestScore = -np.inf #inf bo maksymalizujemy information Gain, gdyby wykorzytstać Gini Index to +inf\n",
    "\t\t\n",
    "\t\tfor feature in range(data.shape[1]):\t\t# dla kazdej cechy\n",
    "\t\t\ttresholds = np.unique(data[:, feature]) # wszytskie mozliwe podzialy\n",
    "\n",
    "\t\t\tfor i in range(1, len(tresholds)):\n",
    "\t\t\t\ttreshold = (tresholds[i - 1] + tresholds[i]) / 2 #podział miedzy punktami danych\n",
    "\t\t\t\tleftMask = data[:, feature] <= treshold # True jesli true dla warunku\n",
    "\t\t\t\trightMask = ~leftMask\n",
    "\t\t\t\n",
    "\t\t\t\tif not np.any(leftMask) or not np.any(rightMask):\n",
    "\t\t\t\t\tcontinue #pomijamy podziały dajace nam pusta grupe\n",
    "\n",
    "\t\t\t\t#leftScore = self.giniIndex(target[leftMask])\n",
    "\t\t\t\t#rightScore = self.giniIndex(target[rightMask])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Jednak zdecydowałam się na entropię, poniewaz daje ona lepsze wyniki na dwóch przetestowanych datasetach\n",
    "\t\t\t\tleftScore = self.entropy(target[leftMask])\n",
    "\t\t\t\trightScore = self.entropy(target[rightMask])\n",
    "\n",
    "\t\t\t\t# cost function -> wazony gini\n",
    "\t\t\t\t#score = (leftScore * np.sum(leftMask) + rightScore * np.sum(rightMask))\n",
    "\n",
    "\t\t\t\tentropy = self.entropy(target)\n",
    "\t\t\t\tinfoGain = entropy - ((np.sum(leftMask) / len(target)) * leftScore + (np.sum(rightMask) / len(target)) * rightScore)\n",
    "\n",
    "\n",
    "\t\t\t\tif infoGain > bestScore:\n",
    "\t\t\t\t\tbestScore = infoGain\n",
    "\t\t\t\t\tbestFeature = feature\n",
    "\t\t\t\t\tbestTreshold = treshold\n",
    "\t\t\n",
    "\t\treturn bestFeature, bestTreshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdef fit(self, data: np.ndarray, target: np.ndarray) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdata (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\t\t\ttarget (np.ndarray): wektor klas o długości n, gdzie n to liczba przykładów\n",
    "\t\t\"\"\"\n",
    "\t\t# Warunek zatrzymania -> jeśli wezel zawiera tylko jedną klase to przypisz ja\n",
    "\t\t# Możnaby dodać np. maksymalną głębokość drzewa, jednak nie wiem na jakich danych będzie wykorzystywana funkcja i nie mam też jeszcze wyczucia na jaką wartość by to można było ustawić\n",
    "\t\tif np.all(target == target[0]):\n",
    "\t\t\tself.prediction = target[0]\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Najlepszy podział\n",
    "\t\tself.splitFeature, self.threshold = self.bestSplit(data, target)\n",
    "\n",
    "\t\tif self.splitFeature is None:\n",
    "\t\t\tself.prediction = np.bincount(target).argmax()\n",
    "\n",
    "\t\tleftMask = data[:, self.splitFeature] <= self.threshold \n",
    "\t\trightMask = ~leftMask\n",
    "\n",
    "\t\tself.left = TreeNode()\n",
    "\t\tself.left.fit(data[leftMask], target[leftMask])\n",
    "\n",
    "\t\tself.right = TreeNode()\n",
    "\t\tself.right.fit(data[rightMask], target[rightMask])\n",
    "\t\t\n",
    "\t\n",
    "\tdef predict(self, data: np.ndarray) -> np.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdata (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tnp.ndarray: wektor przewidzoanych klas o długości n, gdzie n to liczba przykładów\n",
    "\t\t\"\"\"\n",
    "\t\ty_pred = np.zeros(data.shape[0])\n",
    "\n",
    "\t\tfor i, observation in enumerate(data):\n",
    "\t\t\tnode = self\n",
    "\n",
    "\t\t\twhile node.prediction is None:\n",
    "\t\t\t\tnode = node.left if observation[node.splitFeature] <= node.threshold else node.right\n",
    "\n",
    "\t\t\ty_pred[i] = node.prediction\n",
    "\n",
    "\t\treturn y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład trenowanie i testowania drzewa\n",
    " \n",
    "Później znajduje się przykład trenowania i testowania drzewa na zbiorze danych `iris`, który zawierający 150 próbek irysów, z czego każda próbka zawiera 4 atrybuty: długość i szerokość płatków oraz długość i szerokość działki kielicha. Każda próbka należy do jednej z trzech klas: `setosa`, `versicolor` lub `virginica`, które są zakodowane jak int.\n",
    "\n",
    "Możesz go wykorzystać do testowania swojej implementacji. Możesz też zaimplementować własne testy lub użyć innych zbiorów danych, np. innych [zbiorów danych z scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzytanie entropii dla load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867003367003367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie Gini Index dla load_digits\n",
    "\n",
    "Prawdopodobnie z wykorzytaniem Gini Index zaszedł dla tego zbioru danych overfitting, trzeba by zaimplementować pruning. (Lub wykorzystać entropie, która bez pruningu daje okej wyniki na tych dwóch sprawdzonych datasetach.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.3434343434343434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_train)\n",
    "print(accuracy_score(y_train, y_pred))\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
